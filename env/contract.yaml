version: 1
variables:
  APP_ENV:
    type: enum
    enum: [dev, staging, prod]
    required: true
    default: dev
    description: Deployment environment profile.

  NODE_ENV:
    type: enum
    enum: [development, production, test]
    required: true
    default: development
    description: Node.js runtime environment.

  SERVICE_NAME:
    type: string
    required: true
    default: llm-forum
    description: Service name (logical).

  PORT:
    type: int
    required: true
    default: 4000
    description: Backend HTTP server listen port.

  DATABASE_URL:
    type: string
    required: true
    default: postgresql://localhost:5432/llm_forum_dev
    description: PostgreSQL connection URL (Prisma datasource).
    sensitive: true

  CORS_ORIGINS:
    type: string
    required: false
    default: http://localhost:3000
    description: Comma-separated list of allowed CORS origins.

  JWT_SECRET:
    type: string
    required: true
    sensitive: true
    description: Secret key for signing human auth JWT tokens.

  JWT_EXPIRES_IN:
    type: string
    required: false
    default: 7d
    description: JWT token expiration duration.

  SERVICE_AUTH_SECRET:
    type: string
    required: true
    sensitive: true
    description: Shared HMAC secret for Agent Runtime â†” Core Social service-to-service auth.

  VITE_API_URL:
    type: string
    required: false
    default: /v1
    description: Frontend API base URL (Vite env variable, only used in build).

  LLM_PROVIDER:
    type: string
    required: false
    default: openai-compatible
    description: LLM provider identifier.

  LLM_MODEL:
    type: string
    required: false
    default: qwen-plus
    description: Default LLM model name.

  LLM_API_KEY:
    type: string
    required: true
    sensitive: true
    description: API key for the LLM provider.

  LLM_BASE_URL:
    type: string
    required: false
    default: https://dashscope.aliyuncs.com/compatible-mode/v1
    description: Base URL for the LLM API (OpenAI-compatible endpoint).

  LLM_MAX_TOKENS:
    type: int
    required: false
    default: 512
    description: Maximum generation tokens per LLM call.

  LLM_TEMPERATURE:
    type: string
    required: false
    default: "0.8"
    description: LLM generation temperature.

  LLM_MAX_RETRIES:
    type: int
    required: false
    default: 2
    description: Max retry count for failed LLM calls.

  LLM_TIMEOUT_MS:
    type: int
    required: false
    default: 30000
    description: Timeout per LLM API call in milliseconds.

  RUNTIME_ENABLED:
    type: enum
    enum: ["true", "false"]
    required: false
    default: "false"
    description: Enable the Agent Runtime loop on server start.

  RUNTIME_INTERVAL_MS:
    type: int
    required: false
    default: 5000
    description: RuntimeLoop tick interval in milliseconds.

  RUNTIME_BATCH_SIZE:
    type: int
    required: false
    default: 10
    description: Max events processed per RuntimeLoop tick.
